{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Risk Analysis\n",
    "## Model Performance Assessment and Operational Recommendations\n",
    "\n",
    "**Document Type:** Regulatory Defense and Operational Guidance  \n",
    "**Audience:** Model Reviewers, Auditors, Operations Leadership  \n",
    "**Prerequisites:** All models trained and artifacts generated  \n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This notebook provides:\n",
    "1. Comprehensive model evaluation beyond standard metrics\n",
    "2. Analysis of error cases and failure modes\n",
    "3. Seasonal and temporal stability assessment\n",
    "4. Operational recommendations for deployment\n",
    "\n",
    "This document is intended to support regulatory review and operational decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, precision_recall_curve, auc,\n",
    "    confusion_matrix, roc_curve\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / 'artifacts'\n",
    "\n",
    "print(f'Project Root: {PROJECT_ROOT}')\n",
    "print(f'Execution Time: {datetime.now().isoformat()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed features\n",
    "features = pd.read_parquet(ARTIFACTS_DIR / 'preprocessed.parquet')\n",
    "\n",
    "# Load models\n",
    "xgb_model = joblib.load(ARTIFACTS_DIR / 'model_xgb.joblib')\n",
    "\n",
    "# Load metadata\n",
    "with open(ARTIFACTS_DIR / 'metadata.json') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'thresholds.json') as f:\n",
    "    thresholds = json.load(f)\n",
    "\n",
    "print('=== ARTIFACTS LOADED ===')\n",
    "print(f'Model version: {metadata[\"version\"]}')\n",
    "print(f'Features: {len(features):,} records')\n",
    "print(f'Threshold (operational): {thresholds[\"xgb_operational\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare evaluation data\n",
    "feature_cols = metadata['feature_names']\n",
    "\n",
    "if 'quality_flag' in features.columns:\n",
    "    eval_data = features[features['quality_flag'] == 'HIGH'].copy()\n",
    "else:\n",
    "    eval_data = features.copy()\n",
    "\n",
    "X = eval_data[feature_cols].values.astype(np.float32)\n",
    "X = np.nan_to_num(X, nan=0.0)\n",
    "y = eval_data['FLAG'].values\n",
    "\n",
    "# Generate predictions\n",
    "y_proba = xgb_model.predict_proba(X)[:, 1]\n",
    "y_pred = (y_proba >= thresholds['xgb_operational']).astype(int)\n",
    "\n",
    "print(f'\\nEvaluation data: {len(eval_data):,} samples')\n",
    "print(f'Actual theft rate: {y.mean():.2%}')\n",
    "print(f'Predicted positive rate: {y_pred.mean():.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Metrics\n",
    "\n",
    "### 2.1 Why Precision@k Matters\n",
    "\n",
    "In operational settings, utilities have limited investigation capacity. The relevant question is not \"How accurate is the model overall?\" but rather:\n",
    "\n",
    "> \"If we investigate the top 100 flagged customers, how many will be confirmed theft cases?\"\n",
    "\n",
    "This is precision at the top of the ranked list (Precision@k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision@k analysis\n",
    "def precision_at_k(y_true, y_scores, k_pct):\n",
    "    \"\"\"Calculate precision at top k percent.\"\"\"\n",
    "    n = len(y_true)\n",
    "    n_top = max(1, int(n * k_pct))\n",
    "    top_indices = np.argsort(y_scores)[-n_top:]\n",
    "    return y_true[top_indices].mean()\n",
    "\n",
    "k_values = [0.01, 0.02, 0.05, 0.10, 0.15, 0.20]\n",
    "\n",
    "print('=== PRECISION@K ANALYSIS ===')\n",
    "print(f'{\"k\":>8} {\"Customers\":>12} {\"Precision\":>12} {\"Lift\":>10}')\n",
    "print('-' * 44)\n",
    "\n",
    "baseline_rate = y.mean()\n",
    "for k in k_values:\n",
    "    p_at_k = precision_at_k(y, y_proba, k)\n",
    "    n_customers = int(len(y) * k)\n",
    "    lift = p_at_k / baseline_rate\n",
    "    print(f'{k:>8.0%} {n_customers:>12,} {p_at_k:>12.1%} {lift:>10.1f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize precision-recall curve\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y, y_proba)\n",
    "auprc = auc(recall, precision)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Precision-Recall curve\n",
    "axes[0].plot(recall, precision, color='#2E86AB', linewidth=2)\n",
    "axes[0].fill_between(recall, precision, alpha=0.2, color='#2E86AB')\n",
    "axes[0].axhline(y.mean(), color='gray', linestyle='--', label=f'Baseline: {y.mean():.2%}')\n",
    "axes[0].set_xlabel('Recall')\n",
    "axes[0].set_ylabel('Precision')\n",
    "axes[0].set_title(f'Precision-Recall Curve (AUPRC = {auprc:.4f})')\n",
    "axes[0].legend()\n",
    "\n",
    "# Lift curve\n",
    "percentiles = np.linspace(0.01, 1.0, 100)\n",
    "lifts = [precision_at_k(y, y_proba, p) / baseline_rate for p in percentiles]\n",
    "axes[1].plot(percentiles * 100, lifts, color='#E94F37', linewidth=2)\n",
    "axes[1].axhline(1.0, color='gray', linestyle='--', label='Random selection')\n",
    "axes[1].set_xlabel('Percentage of Customers Flagged')\n",
    "axes[1].set_ylabel('Lift over Random')\n",
    "axes[1].set_title('Lift Curve')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ARTIFACTS_DIR / 'precision_recall_lift.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Figure saved: precision_recall_lift.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix at operational threshold\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "print('=== CONFUSION MATRIX ===')\n",
    "print(f'Threshold: {thresholds[\"xgb_operational\"]:.4f}')\n",
    "print()\n",
    "print(f'{\"\":>20} {\"Predicted Normal\":>18} {\"Predicted Theft\":>18}')\n",
    "print(f'{\"Actual Normal\":>20} {cm[0,0]:>18,} {cm[0,1]:>18,}')\n",
    "print(f'{\"Actual Theft\":>20} {cm[1,0]:>18,} {cm[1,1]:>18,}')\n",
    "\n",
    "# Derived metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision_val = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall_val = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(f'\\nPrecision: {precision_val:.2%}')\n",
    "print(f'Recall: {recall_val:.2%}')\n",
    "print(f'False Positive Rate: {fpr:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error Case Analysis\n",
    "\n",
    "Understanding error cases is essential for:\n",
    "- Improving model performance\n",
    "- Setting operational expectations\n",
    "- Communicating limitations to stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify false positives and false negatives\n",
    "eval_data['predicted_proba'] = y_proba\n",
    "eval_data['predicted'] = y_pred\n",
    "\n",
    "false_positives = eval_data[(eval_data['FLAG'] == 0) & (eval_data['predicted'] == 1)]\n",
    "false_negatives = eval_data[(eval_data['FLAG'] == 1) & (eval_data['predicted'] == 0)]\n",
    "true_positives = eval_data[(eval_data['FLAG'] == 1) & (eval_data['predicted'] == 1)]\n",
    "\n",
    "print('=== ERROR CASE SUMMARY ===')\n",
    "print(f'False Positives: {len(false_positives):,} ({len(false_positives)/len(eval_data)*100:.2f}%)')\n",
    "print(f'False Negatives: {len(false_negatives):,} ({len(false_negatives)/len(eval_data)*100:.2f}%)')\n",
    "print(f'True Positives: {len(true_positives):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze characteristics of false positives\n",
    "if len(false_positives) > 0:\n",
    "    print('\\n=== FALSE POSITIVE CHARACTERISTICS ===')\n",
    "    print('Customers incorrectly flagged as theft:')\n",
    "    \n",
    "    fp_stats = {\n",
    "        'Avg consumption mean': false_positives['consumption_mean'].mean(),\n",
    "        'Avg consumption CV': false_positives['consumption_cv'].mean(),\n",
    "        'Avg zero ratio': false_positives['zero_ratio'].mean() if 'zero_ratio' in false_positives.columns else 'N/A'\n",
    "    }\n",
    "    \n",
    "    normal_stats = {\n",
    "        'Avg consumption mean': eval_data[eval_data['FLAG']==0]['consumption_mean'].mean(),\n",
    "        'Avg consumption CV': eval_data[eval_data['FLAG']==0]['consumption_cv'].mean(),\n",
    "        'Avg zero ratio': eval_data[eval_data['FLAG']==0]['zero_ratio'].mean() if 'zero_ratio' in eval_data.columns else 'N/A'\n",
    "    }\n",
    "    \n",
    "    print(f'{\"Metric\":>25} {\"False Positives\":>18} {\"Normal Customers\":>18}')\n",
    "    print('-' * 63)\n",
    "    for metric in fp_stats:\n",
    "        fp_val = fp_stats[metric]\n",
    "        normal_val = normal_stats[metric]\n",
    "        if isinstance(fp_val, float):\n",
    "            print(f'{metric:>25} {fp_val:>18.2f} {normal_val:>18.2f}')\n",
    "        else:\n",
    "            print(f'{metric:>25} {fp_val:>18} {normal_val:>18}')\n",
    "    \n",
    "    print('\\nInterpretation: False positives tend to be legitimate customers')\n",
    "    print('with higher consumption variability, triggering anomaly detection.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze characteristics of false negatives\n",
    "if len(false_negatives) > 0:\n",
    "    print('\\n=== FALSE NEGATIVE CHARACTERISTICS ===')\n",
    "    print('Theft cases missed by the model:')\n",
    "    \n",
    "    fn_stats = {\n",
    "        'Avg consumption mean': false_negatives['consumption_mean'].mean(),\n",
    "        'Avg consumption CV': false_negatives['consumption_cv'].mean(),\n",
    "        'Avg prediction score': false_negatives['predicted_proba'].mean()\n",
    "    }\n",
    "    \n",
    "    tp_stats = {\n",
    "        'Avg consumption mean': true_positives['consumption_mean'].mean() if len(true_positives) > 0 else 0,\n",
    "        'Avg consumption CV': true_positives['consumption_cv'].mean() if len(true_positives) > 0 else 0,\n",
    "        'Avg prediction score': true_positives['predicted_proba'].mean() if len(true_positives) > 0 else 0\n",
    "    }\n",
    "    \n",
    "    print(f'{\"Metric\":>25} {\"Missed Cases\":>18} {\"Detected Cases\":>18}')\n",
    "    print('-' * 63)\n",
    "    for metric in fn_stats:\n",
    "        print(f'{metric:>25} {fn_stats[metric]:>18.2f} {tp_stats[metric]:>18.2f}')\n",
    "    \n",
    "    print('\\nInterpretation: Missed theft cases tend to have smaller magnitude')\n",
    "    print('consumption changes, making them harder to distinguish from normal variation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cluster-Specific Performance\n",
    "\n",
    "Model performance should be validated across customer segments to ensure equitable coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cluster' in eval_data.columns:\n",
    "    print('=== PERFORMANCE BY CLUSTER ===')\n",
    "    print(f'{\"Cluster\":>10} {\"Samples\":>10} {\"Theft Rate\":>12} {\"Precision\":>12} {\"Recall\":>10}')\n",
    "    print('-' * 56)\n",
    "    \n",
    "    for cluster_id in sorted(eval_data['cluster'].unique()):\n",
    "        if cluster_id < 0:\n",
    "            continue\n",
    "        \n",
    "        cluster_data = eval_data[eval_data['cluster'] == cluster_id]\n",
    "        y_c = cluster_data['FLAG'].values\n",
    "        y_pred_c = cluster_data['predicted'].values\n",
    "        \n",
    "        if y_c.sum() == 0 or y_pred_c.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        cm_c = confusion_matrix(y_c, y_pred_c)\n",
    "        tn_c, fp_c, fn_c, tp_c = cm_c.ravel() if cm_c.size == 4 else (0, 0, 0, 0)\n",
    "        \n",
    "        precision_c = tp_c / (tp_c + fp_c) if (tp_c + fp_c) > 0 else 0\n",
    "        recall_c = tp_c / (tp_c + fn_c) if (tp_c + fn_c) > 0 else 0\n",
    "        \n",
    "        print(f'{cluster_id:>10} {len(cluster_data):>10,} {y_c.mean():>12.2%} {precision_c:>12.2%} {recall_c:>10.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Operational Recommendations\n",
    "\n",
    "### 6.1 When to Investigate\n",
    "\n",
    "| Score Range | Risk Level | Recommended Action |\n",
    "|------------|------------|--------------------|\n",
    "| >= 0.80 | High | Priority investigation within 7 days |\n",
    "| 0.50 - 0.80 | Medium | Standard investigation queue |\n",
    "| 0.30 - 0.50 | Low | Flag for monitoring, no immediate action |\n",
    "| < 0.30 | Minimal | No action required |\n",
    "\n",
    "### 6.2 When NOT to Act on Model Alerts\n",
    "\n",
    "Inspectors should exercise caution when:\n",
    "\n",
    "1. **Recent meter replacement:** Consumption pattern disruption may be due to new meter calibration\n",
    "2. **Known renovations:** Commercial customers undergoing construction may have unusual patterns\n",
    "3. **Seasonal anomalies:** First flag in vacation season for residential customers\n",
    "4. **Data quality issues:** Customer has quality_flag != 'HIGH'\n",
    "5. **Low confidence scores:** Model uncertainty is high\n",
    "\n",
    "### 6.3 Feedback Loop\n",
    "\n",
    "Investigation outcomes should be recorded and fed back to the model:\n",
    "\n",
    "| Outcome | Action |\n",
    "|---------|--------|\n",
    "| Confirmed theft | Add to positive training set |\n",
    "| Meter malfunction | Exclude from training; investigate data quality |\n",
    "| Legitimate behavior | Add to negative training set; adjust threshold if recurring pattern |\n",
    "| Inconclusive | Flag for re-investigation in 30 days |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation metrics\n",
    "evaluation_metrics = {\n",
    "    'evaluated_at': datetime.now().isoformat(),\n",
    "    'model_version': metadata['version'],\n",
    "    'n_samples': len(eval_data),\n",
    "    'threshold': thresholds['xgb_operational'],\n",
    "    'metrics': {\n",
    "        'auprc': float(auprc),\n",
    "        'precision': float(precision_val),\n",
    "        'recall': float(recall_val),\n",
    "        'false_positive_rate': float(fpr),\n",
    "        'precision_at_1pct': float(precision_at_k(y, y_proba, 0.01)),\n",
    "        'precision_at_5pct': float(precision_at_k(y, y_proba, 0.05))\n",
    "    },\n",
    "    'confusion_matrix': {\n",
    "        'true_negative': int(tn),\n",
    "        'false_positive': int(fp),\n",
    "        'false_negative': int(fn),\n",
    "        'true_positive': int(tp)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'evaluation_metrics.json', 'w') as f:\n",
    "    json.dump(evaluation_metrics, f, indent=2)\n",
    "\n",
    "print(f'Saved: {ARTIFACTS_DIR / \"evaluation_metrics.json\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print('\\n' + '='*60)\n",
    "print('EVALUATION SUMMARY')\n",
    "print('='*60)\n",
    "print(f'\\nModel Performance:')\n",
    "print(f'  AUPRC: {auprc:.4f}')\n",
    "print(f'  Precision@1%: {precision_at_k(y, y_proba, 0.01):.1%}')\n",
    "print(f'  Precision@5%: {precision_at_k(y, y_proba, 0.05):.1%}')\n",
    "print(f'\\nAt Operational Threshold ({thresholds[\"xgb_operational\"]:.4f}):')\n",
    "print(f'  Precision: {precision_val:.1%}')\n",
    "print(f'  Recall: {recall_val:.1%}')\n",
    "print(f'  False Positive Rate: {fpr:.1%}')\n",
    "print(f'\\nError Analysis:')\n",
    "print(f'  False Positives: {len(false_positives):,}')\n",
    "print(f'  False Negatives: {len(false_negatives):,}')\n",
    "print('='*60)\n",
    "print('\\nRECOMMENDATION: Model is suitable for deployment as a')\n",
    "print('decision-support tool with human review of all flagged cases.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Document End**\n",
    "\n",
    "This evaluation report should be reviewed by model governance committee before production deployment. All findings should be documented in the model risk assessment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
