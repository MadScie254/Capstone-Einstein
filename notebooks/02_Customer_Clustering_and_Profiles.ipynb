{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Clustering and Profile Analysis\n",
    "## Behavioral Segmentation for NTL Detection\n",
    "\n",
    "**Document Type:** Technical Analysis Report  \n",
    "**Audience:** Data Scientists, Utility Analysts, Operations Teams  \n",
    "**Prerequisites:** Review 01_EDA_and_Preprocessing.ipynb; artifacts must be generated  \n",
    "\n",
    "---\n",
    "\n",
    "### Rationale\n",
    "\n",
    "A single anomaly detection model applied uniformly across all customers produces suboptimal results due to customer heterogeneity. This notebook establishes behavioral clusters to enable:\n",
    "\n",
    "1. **Baseline calibration:** Define \"normal\" relative to peer group, not global population\n",
    "2. **Threshold customization:** Set cluster-specific detection thresholds\n",
    "3. **Interpretability:** Explain flags in context of customer segment\n",
    "4. **Operational routing:** Route alerts to appropriate investigation teams\n",
    "\n",
    "### Outputs\n",
    "\n",
    "- `artifacts/cluster_model.joblib` - Trained clustering model\n",
    "- `artifacts/cluster_profiles.json` - Cluster characteristics and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / 'artifacts'\n",
    "\n",
    "print(f'Project Root: {PROJECT_ROOT}')\n",
    "print(f'Execution Time: {datetime.now().isoformat()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features\n",
    "features = pd.read_parquet(ARTIFACTS_DIR / 'preprocessed.parquet')\n",
    "\n",
    "print('=== DATA LOADED ===')\n",
    "print(f'Records: {len(features):,}')\n",
    "print(f'Features: {len(features.columns)}')\n",
    "\n",
    "# Filter to high-quality records only for clustering\n",
    "if 'quality_flag' in features.columns:\n",
    "    high_quality = features[features['quality_flag'] == 'HIGH'].copy()\n",
    "    print(f'High-quality records for clustering: {len(high_quality):,}')\n",
    "else:\n",
    "    high_quality = features.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection for Clustering\n",
    "\n",
    "Clustering features should capture consumption behavior, not anomaly indicators. We exclude:\n",
    "- Anomaly-specific features (zero_ratio, sudden_drop_count)\n",
    "- Metadata (CONS_NO, FLAG, quality_flag)\n",
    "\n",
    "This ensures clusters represent natural customer segments, not theft/non-theft groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select clustering features\n",
    "exclude_features = [\n",
    "    'CONS_NO', 'FLAG', 'quality_flag',\n",
    "    'zero_ratio', 'sudden_drop_count', 'sudden_drop_pct',\n",
    "    'max_consecutive_zeros', 'missing_ratio', 'is_constant',\n",
    "    'outlier_ratio', 'low_consumption_ratio'\n",
    "]\n",
    "\n",
    "cluster_features = [\n",
    "    'consumption_mean', 'consumption_std', 'consumption_cv',\n",
    "    'consumption_median', 'consumption_range',\n",
    "    'autocorrelation', 'consumption_trend',\n",
    "    'rolling_7d_mean_avg', 'weekly_pattern_strength'\n",
    "]\n",
    "\n",
    "# Filter to available features\n",
    "cluster_features = [f for f in cluster_features if f in high_quality.columns]\n",
    "\n",
    "print('=== CLUSTERING FEATURES ===')\n",
    "for f in cluster_features:\n",
    "    print(f'  - {f}')\n",
    "\n",
    "X_cluster = high_quality[cluster_features].values\n",
    "print(f'\\nFeature matrix shape: {X_cluster.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values and scale\n",
    "X_cluster = np.nan_to_num(X_cluster, nan=0.0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_cluster)\n",
    "\n",
    "print('Features scaled to zero mean, unit variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimal Cluster Selection\n",
    "\n",
    "We evaluate cluster counts using:\n",
    "- **Silhouette Score:** Measures cohesion vs. separation\n",
    "- **Calinski-Harabasz Index:** Ratio of between-cluster to within-cluster variance\n",
    "- **Business interpretability:** Clusters must map to actionable segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate cluster counts\n",
    "k_range = range(2, 8)\n",
    "silhouette_scores = []\n",
    "ch_scores = []\n",
    "inertias = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    silhouette_scores.append(silhouette_score(X_scaled, labels))\n",
    "    ch_scores.append(calinski_harabasz_score(X_scaled, labels))\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Display results\n",
    "print('=== CLUSTER EVALUATION ===')\n",
    "print(f'{\"k\":>3} {\"Silhouette\":>12} {\"Calinski-H\":>12} {\"Inertia\":>12}')\n",
    "print('-' * 42)\n",
    "for k, sil, ch, iner in zip(k_range, silhouette_scores, ch_scores, inertias):\n",
    "    print(f'{k:3d} {sil:12.4f} {ch:12.1f} {iner:12.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(list(k_range), silhouette_scores, 'o-', color='#2E86AB')\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Silhouette Score')\n",
    "axes[0].set_title('Silhouette Score by Cluster Count')\n",
    "\n",
    "axes[1].plot(list(k_range), ch_scores, 's-', color='#E94F37')\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Calinski-Harabasz Index')\n",
    "axes[1].set_title('Calinski-Harabasz Index by Cluster Count')\n",
    "\n",
    "axes[2].plot(list(k_range), inertias, '^-', color='#4ECDC4')\n",
    "axes[2].set_xlabel('Number of Clusters (k)')\n",
    "axes[2].set_ylabel('Inertia')\n",
    "axes[2].set_title('Elbow Plot (Inertia)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ARTIFACTS_DIR / 'cluster_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Figure saved: cluster_evaluation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select optimal k (typically 3-5 for utility customers)\n",
    "OPTIMAL_K = 4  # Based on silhouette and interpretability\n",
    "\n",
    "print(f'\\nSelected cluster count: k = {OPTIMAL_K}')\n",
    "print('Rationale: Balances statistical metrics with operational interpretability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Final Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model\n",
    "cluster_model = KMeans(n_clusters=OPTIMAL_K, random_state=42, n_init=10)\n",
    "high_quality['cluster'] = cluster_model.fit_predict(X_scaled)\n",
    "\n",
    "# Assign clusters to all records\n",
    "features['cluster'] = -1  # Default for low-quality records\n",
    "features.loc[high_quality.index, 'cluster'] = high_quality['cluster']\n",
    "\n",
    "print('=== CLUSTER DISTRIBUTION ===')\n",
    "print(features['cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cluster Profile Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cluster profiles\n",
    "profile_features = ['consumption_mean', 'consumption_std', 'consumption_cv']\n",
    "if 'zero_ratio' in features.columns:\n",
    "    profile_features.append('zero_ratio')\n",
    "\n",
    "cluster_profiles = high_quality.groupby('cluster')[profile_features].agg(['mean', 'std'])\n",
    "\n",
    "print('=== CLUSTER PROFILES ===')\n",
    "for cluster_id in range(OPTIMAL_K):\n",
    "    cluster_data = high_quality[high_quality['cluster'] == cluster_id]\n",
    "    print(f'\\nCluster {cluster_id} (n = {len(cluster_data):,}):')\n",
    "    print(f'  Mean consumption: {cluster_data[\"consumption_mean\"].mean():.1f} kWh')\n",
    "    print(f'  Consumption variability (CV): {cluster_data[\"consumption_cv\"].mean():.2f}')\n",
    "    if 'FLAG' in cluster_data.columns:\n",
    "        print(f'  Theft rate: {cluster_data[\"FLAG\"].mean():.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster characteristics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Consumption by cluster\n",
    "cluster_means = high_quality.groupby('cluster')['consumption_mean'].mean()\n",
    "axes[0].bar(cluster_means.index, cluster_means.values, color=['#2E86AB', '#E94F37', '#4ECDC4', '#F7B32B'][:OPTIMAL_K])\n",
    "axes[0].set_xlabel('Cluster')\n",
    "axes[0].set_ylabel('Mean Consumption (kWh)')\n",
    "axes[0].set_title('Average Consumption by Cluster')\n",
    "\n",
    "# Variability by cluster\n",
    "cluster_cv = high_quality.groupby('cluster')['consumption_cv'].mean()\n",
    "axes[1].bar(cluster_cv.index, cluster_cv.values, color=['#2E86AB', '#E94F37', '#4ECDC4', '#F7B32B'][:OPTIMAL_K])\n",
    "axes[1].set_xlabel('Cluster')\n",
    "axes[1].set_ylabel('Coefficient of Variation')\n",
    "axes[1].set_title('Consumption Variability by Cluster')\n",
    "\n",
    "# Theft rate by cluster (if labels available)\n",
    "if 'FLAG' in high_quality.columns:\n",
    "    cluster_theft = high_quality.groupby('cluster')['FLAG'].mean()\n",
    "    axes[2].bar(cluster_theft.index, cluster_theft.values * 100, color=['#2E86AB', '#E94F37', '#4ECDC4', '#F7B32B'][:OPTIMAL_K])\n",
    "    axes[2].set_xlabel('Cluster')\n",
    "    axes[2].set_ylabel('Theft Rate (%)')\n",
    "    axes[2].set_title('Theft Rate by Cluster')\n",
    "    axes[2].axhline(high_quality['FLAG'].mean() * 100, color='red', linestyle='--', label='Overall rate')\n",
    "    axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ARTIFACTS_DIR / 'cluster_profiles.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Figure saved: cluster_profiles.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Business Interpretation\n",
    "\n",
    "Assign meaningful labels to each cluster based on consumption characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cluster descriptions\n",
    "cluster_descriptions = {}\n",
    "\n",
    "for cluster_id in range(OPTIMAL_K):\n",
    "    cluster_data = high_quality[high_quality['cluster'] == cluster_id]\n",
    "    mean_consumption = cluster_data['consumption_mean'].mean()\n",
    "    cv = cluster_data['consumption_cv'].mean()\n",
    "    \n",
    "    # Classify based on consumption level and variability\n",
    "    if mean_consumption < 50:\n",
    "        level = 'Low'\n",
    "    elif mean_consumption < 150:\n",
    "        level = 'Medium'\n",
    "    else:\n",
    "        level = 'High'\n",
    "    \n",
    "    if cv < 0.2:\n",
    "        variability = 'Stable'\n",
    "    elif cv < 0.5:\n",
    "        variability = 'Moderate'\n",
    "    else:\n",
    "        variability = 'Variable'\n",
    "    \n",
    "    cluster_descriptions[cluster_id] = {\n",
    "        'name': f'{level} Consumption, {variability} Pattern',\n",
    "        'mean_consumption': float(mean_consumption),\n",
    "        'cv': float(cv),\n",
    "        'n_customers': len(cluster_data),\n",
    "        'theft_rate': float(cluster_data['FLAG'].mean()) if 'FLAG' in cluster_data.columns else None\n",
    "    }\n",
    "\n",
    "print('=== CLUSTER INTERPRETATION ===')\n",
    "for cluster_id, desc in cluster_descriptions.items():\n",
    "    print(f'\\nCluster {cluster_id}: {desc[\"name\"]}')\n",
    "    print(f'  Customers: {desc[\"n_customers\"]:,}')\n",
    "    print(f'  Avg consumption: {desc[\"mean_consumption\"]:.1f} kWh')\n",
    "    if desc['theft_rate'] is not None:\n",
    "        print(f'  Theft rate: {desc[\"theft_rate\"]:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cluster model\n",
    "joblib.dump({\n",
    "    'model': cluster_model,\n",
    "    'scaler': scaler,\n",
    "    'features': cluster_features,\n",
    "    'n_clusters': OPTIMAL_K\n",
    "}, ARTIFACTS_DIR / 'cluster_model.joblib')\n",
    "print(f'Saved: {ARTIFACTS_DIR / \"cluster_model.joblib\"}')\n",
    "\n",
    "# Save cluster profiles\n",
    "cluster_profiles_output = {\n",
    "    'n_clusters': OPTIMAL_K,\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'profiles': cluster_descriptions\n",
    "}\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'cluster_profiles.json', 'w') as f:\n",
    "    json.dump(cluster_profiles_output, f, indent=2)\n",
    "print(f'Saved: {ARTIFACTS_DIR / \"cluster_profiles.json\"}')\n",
    "\n",
    "# Update preprocessed data with cluster assignments\n",
    "features.to_parquet(ARTIFACTS_DIR / 'preprocessed.parquet', index=False)\n",
    "print(f'Updated: {ARTIFACTS_DIR / \"preprocessed.parquet\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Risks of Mis-clustering\n",
    "\n",
    "### Potential Issues\n",
    "\n",
    "1. **Cluster drift over time:** Seasonal changes may shift customers between clusters. Recommend quarterly cluster reassignment.\n",
    "\n",
    "2. **Boundary customers:** Customers near cluster boundaries may be assigned differently on re-clustering. Consider soft clustering (GMM) for high-stakes decisions.\n",
    "\n",
    "3. **New customer types:** Market changes (e.g., EV adoption, solar panels) may create customers that fit no existing cluster.\n",
    "\n",
    "4. **Theft-induced cluster shifts:** Customers engaged in theft may appear as a different consumption profile. This is a feature, not a bug, but requires careful threshold calibration.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step:** Proceed to 03_Anomaly_and_Theft_Models.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
