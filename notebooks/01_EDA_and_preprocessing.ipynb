{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Preprocessing\n",
    "## Non-Technical Loss Detection Pipeline\n",
    "\n",
    "**Document Type:** Technical Analysis Report  \n",
    "**Audience:** Data Scientists, ML Engineers, Technical Reviewers  \n",
    "**Prerequisites:** Review 00_Project_Context_and_Problem_Definition.ipynb  \n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "\n",
    "This notebook performs data validation, quality assessment, and feature engineering for the NTL detection pipeline. All transformations are deterministic and versioned for reproducibility.\n",
    "\n",
    "### Outputs\n",
    "\n",
    "- `artifacts/preprocessed.parquet` - Feature matrix for modeling\n",
    "- `artifacts/pipeline_v1.joblib` - Serialized preprocessing pipeline\n",
    "- `artifacts/feature_schema.json` - Feature specifications and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / 'artifacts'\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Add backend to path for preprocessing utilities\n",
    "sys.path.insert(0, str(PROJECT_ROOT / 'src' / 'backend'))\n",
    "\n",
    "print(f'Project Root: {PROJECT_ROOT}')\n",
    "print(f'Data Directory: {DATA_DIR}')\n",
    "print(f'Artifacts Directory: {ARTIFACTS_DIR}')\n",
    "print(f'Execution Time: {datetime.now().isoformat()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion and Validation\n",
    "\n",
    "### 1.1 Schema Verification\n",
    "\n",
    "Expected data format:\n",
    "- Each row represents one customer meter\n",
    "- Columns contain daily consumption readings (kWh)\n",
    "- `CONS_NO`: Unique customer/meter identifier\n",
    "- `FLAG`: Binary label (0 = normal, 1 = confirmed theft) where available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_file = DATA_DIR / 'datasetsmall.csv'\n",
    "df_raw = pd.read_csv(data_file)\n",
    "\n",
    "print('=== DATA INGESTION SUMMARY ===')\n",
    "print(f'Source file: {data_file.name}')\n",
    "print(f'Total records: {len(df_raw):,}')\n",
    "print(f'Total columns: {len(df_raw.columns)}')\n",
    "print(f'Memory usage: {df_raw.memory_usage(deep=True).sum() / 1e6:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify column types\n",
    "metadata_cols = ['CONS_NO', 'FLAG']\n",
    "consumption_cols = [c for c in df_raw.columns if c not in metadata_cols]\n",
    "\n",
    "print(f'\\n=== COLUMN STRUCTURE ===')\n",
    "print(f'Metadata columns: {metadata_cols}')\n",
    "print(f'Consumption columns: {len(consumption_cols)}')\n",
    "print(f'Date range: {consumption_cols[0]} to {consumption_cols[-1]}')\n",
    "\n",
    "# Validate expected columns exist\n",
    "missing_cols = [c for c in metadata_cols if c not in df_raw.columns]\n",
    "if missing_cols:\n",
    "    print(f'WARNING: Missing expected columns: {missing_cols}')\n",
    "else:\n",
    "    print('Schema validation: PASSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution analysis\n",
    "if 'FLAG' in df_raw.columns:\n",
    "    label_counts = df_raw['FLAG'].value_counts().sort_index()\n",
    "    theft_ratio = df_raw['FLAG'].mean()\n",
    "    \n",
    "    print('\\n=== LABEL DISTRIBUTION ===')\n",
    "    print(f'Normal (FLAG=0): {label_counts.get(0, 0):,}')\n",
    "    print(f'Theft (FLAG=1): {label_counts.get(1, 0):,}')\n",
    "    print(f'Theft ratio: {theft_ratio:.2%}')\n",
    "    print(f'Class imbalance ratio: 1:{int(1/theft_ratio) if theft_ratio > 0 else \"inf\"}')\n",
    "else:\n",
    "    print('WARNING: No label column found. Unsupervised methods only.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Missingness Analysis\n",
    "\n",
    "Understanding data gaps is critical for:\n",
    "- Imputation strategy selection\n",
    "- Feature reliability assessment\n",
    "- Identifying potential meter communication issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert consumption columns to numeric, coercing errors\n",
    "df = df_raw.copy()\n",
    "for col in consumption_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Calculate missingness\n",
    "consumption_matrix = df[consumption_cols].values\n",
    "missing_count = np.isnan(consumption_matrix).sum()\n",
    "total_values = consumption_matrix.size\n",
    "missing_pct = missing_count / total_values * 100\n",
    "\n",
    "print('=== MISSINGNESS ANALYSIS ===')\n",
    "print(f'Total values: {total_values:,}')\n",
    "print(f'Missing values: {missing_count:,}')\n",
    "print(f'Missing percentage: {missing_pct:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-customer missingness\n",
    "missing_per_customer = df[consumption_cols].isna().sum(axis=1)\n",
    "n_days = len(consumption_cols)\n",
    "\n",
    "print('\\n=== PER-CUSTOMER MISSINGNESS ===')\n",
    "print(f'Customers with complete data: {(missing_per_customer == 0).sum():,}')\n",
    "print(f'Customers with any missing: {(missing_per_customer > 0).sum():,}')\n",
    "print(f'Customers with >50% missing: {(missing_per_customer > n_days * 0.5).sum():,}')\n",
    "print(f'Average missing days per customer: {missing_per_customer.mean():.2f}')\n",
    "print(f'Maximum missing days: {missing_per_customer.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missingness distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of missing days per customer\n",
    "axes[0].hist(missing_per_customer, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(missing_per_customer.mean(), color='red', linestyle='--', label=f'Mean: {missing_per_customer.mean():.1f}')\n",
    "axes[0].set_xlabel('Number of Missing Days')\n",
    "axes[0].set_ylabel('Number of Customers')\n",
    "axes[0].set_title('Distribution of Missing Data per Customer')\n",
    "axes[0].legend()\n",
    "\n",
    "# Missing by day (temporal pattern)\n",
    "missing_by_day = df[consumption_cols].isna().sum(axis=0)\n",
    "axes[1].bar(range(len(missing_by_day)), missing_by_day, alpha=0.7)\n",
    "axes[1].set_xlabel('Day Index')\n",
    "axes[1].set_ylabel('Number of Missing Values')\n",
    "axes[1].set_title('Missing Data by Day (Temporal Pattern)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ARTIFACTS_DIR / 'missingness_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Figure saved: missingness_analysis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Consumption Pattern Analysis\n",
    "\n",
    "### 2.1 Zero and Near-Zero Consumption\n",
    "\n",
    "Zero consumption readings warrant special attention:\n",
    "- **Legitimate:** Vacation, unoccupied property, meter maintenance\n",
    "- **Suspicious:** Extended zeros in occupied buildings, post-intervention zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero consumption analysis\n",
    "zero_threshold = 0.01  # kWh - readings below this considered zero\n",
    "consumption_values = df[consumption_cols].values.astype(float)\n",
    "\n",
    "# Count zeros per customer\n",
    "zeros_per_customer = np.sum(consumption_values < zero_threshold, axis=1)\n",
    "zeros_per_customer[np.isnan(consumption_values).all(axis=1)] = np.nan  # Handle all-NaN rows\n",
    "\n",
    "print('=== ZERO CONSUMPTION ANALYSIS ===')\n",
    "print(f'Zero threshold: {zero_threshold} kWh')\n",
    "print(f'Customers with any zero days: {(zeros_per_customer > 0).sum():,}')\n",
    "print(f'Customers with >5 zero days: {(zeros_per_customer > 5).sum():,}')\n",
    "print(f'Average zero days per customer: {np.nanmean(zeros_per_customer):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare zero rates between normal and theft groups\n",
    "if 'FLAG' in df.columns:\n",
    "    normal_zeros = zeros_per_customer[df['FLAG'] == 0]\n",
    "    theft_zeros = zeros_per_customer[df['FLAG'] == 1]\n",
    "    \n",
    "    print('\\n=== ZERO DAYS BY CLASS ===')\n",
    "    print(f'Normal customers - mean zero days: {np.nanmean(normal_zeros):.2f}')\n",
    "    print(f'Theft customers - mean zero days: {np.nanmean(theft_zeros):.2f}')\n",
    "    print(f'Ratio (theft/normal): {np.nanmean(theft_zeros) / np.nanmean(normal_zeros):.2f}x')\n",
    "    \n",
    "    # Statistical test\n",
    "    stat, pvalue = stats.mannwhitneyu(normal_zeros[~np.isnan(normal_zeros)], \n",
    "                                       theft_zeros[~np.isnan(theft_zeros)])\n",
    "    print(f'Mann-Whitney U test p-value: {pvalue:.2e}')\n",
    "    print(f'Conclusion: {\"Significant difference\" if pvalue < 0.05 else \"No significant difference\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Consumption Distribution Comparison\n",
    "\n",
    "Visual comparison of consumption patterns between classes provides intuition for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics per customer\n",
    "df['consumption_mean'] = df[consumption_cols].mean(axis=1)\n",
    "df['consumption_std'] = df[consumption_cols].std(axis=1)\n",
    "df['consumption_cv'] = df['consumption_std'] / (df['consumption_mean'] + 1e-10)\n",
    "\n",
    "# Visualize distributions by class\n",
    "if 'FLAG' in df.columns:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    metrics = ['consumption_mean', 'consumption_std', 'consumption_cv']\n",
    "    titles = ['Mean Daily Consumption (kWh)', 'Std Dev of Consumption (kWh)', 'Coefficient of Variation']\n",
    "    \n",
    "    for ax, metric, title in zip(axes, metrics, titles):\n",
    "        for flag, label, color in [(0, 'Normal', '#2E86AB'), (1, 'Theft', '#E94F37')]:\n",
    "            subset = df[df['FLAG'] == flag][metric].dropna()\n",
    "            ax.hist(subset, bins=50, alpha=0.6, label=label, color=color, density=True)\n",
    "        ax.set_xlabel(title)\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_title(f'Distribution: {title}')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ARTIFACTS_DIR / 'consumption_distributions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Figure saved: consumption_distributions.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Temporal Pattern Examples\n",
    "\n",
    "Visualizing individual consumption profiles helps build intuition for normal vs. suspicious patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample and plot consumption profiles\n",
    "np.random.seed(42)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Sample normal customers\n",
    "normal_sample = df[df['FLAG'] == 0].sample(3, random_state=42)\n",
    "for idx, (ax, (_, row)) in enumerate(zip(axes[0], normal_sample.iterrows())):\n",
    "    consumption = row[consumption_cols].values.astype(float)\n",
    "    days = range(len(consumption))\n",
    "    ax.plot(days, consumption, color='#2E86AB', linewidth=1.5)\n",
    "    ax.fill_between(days, consumption, alpha=0.3, color='#2E86AB')\n",
    "    ax.axhline(np.nanmean(consumption), color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_title(f'Normal Customer {idx+1}', fontsize=11)\n",
    "    ax.set_xlabel('Day')\n",
    "    ax.set_ylabel('Consumption (kWh)')\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "# Sample theft customers\n",
    "theft_sample = df[df['FLAG'] == 1].sample(3, random_state=42)\n",
    "for idx, (ax, (_, row)) in enumerate(zip(axes[1], theft_sample.iterrows())):\n",
    "    consumption = row[consumption_cols].values.astype(float)\n",
    "    days = range(len(consumption))\n",
    "    ax.plot(days, consumption, color='#E94F37', linewidth=1.5)\n",
    "    ax.fill_between(days, consumption, alpha=0.3, color='#E94F37')\n",
    "    ax.axhline(np.nanmean(consumption), color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_title(f'Theft Customer {idx+1}', fontsize=11)\n",
    "    ax.set_xlabel('Day')\n",
    "    ax.set_ylabel('Consumption (kWh)')\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "plt.suptitle('Consumption Patterns: Normal vs. Theft Customers', fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ARTIFACTS_DIR / 'consumption_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Figure saved: consumption_patterns.png')\n",
    "print('\\nObservation: Theft customers often exhibit sudden drops, extended zeros, or unusual volatility.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "### 3.1 Feature Design Rationale\n",
    "\n",
    "Features are designed to capture:\n",
    "\n",
    "| Feature Category | Rationale | Examples |\n",
    "|-----------------|-----------|----------|\n",
    "| **Central Tendency** | Baseline consumption level | Mean, median |\n",
    "| **Volatility** | Consumption stability | Std, CV, range |\n",
    "| **Anomaly Indicators** | Theft-specific patterns | Zero ratio, sudden drops |\n",
    "| **Temporal Patterns** | Behavioral consistency | Weekly correlation, trend |\n",
    "| **Rolling Statistics** | Recent behavior changes | 7-day mean, 14-day std |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessing pipeline\n",
    "from app.preprocess import FeaturePipeline, FeatureConfig\n",
    "\n",
    "# Configure pipeline\n",
    "config = FeatureConfig(\n",
    "    rolling_windows=[3, 7, 14],\n",
    "    zero_threshold=0.01,\n",
    "    min_valid_days=10\n",
    ")\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = FeaturePipeline(config=config)\n",
    "\n",
    "print('=== FEATURE ENGINEERING CONFIGURATION ===')\n",
    "print(f'Pipeline version: {pipeline.version}')\n",
    "print(f'Rolling windows: {config.rolling_windows} days')\n",
    "print(f'Zero threshold: {config.zero_threshold} kWh')\n",
    "print(f'Minimum valid days: {config.min_valid_days}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform\n",
    "print('\\nExecuting feature engineering pipeline...')\n",
    "features = pipeline.fit_transform(df_raw)\n",
    "\n",
    "print('\\n=== FEATURE ENGINEERING RESULTS ===')\n",
    "print(f'Input shape: {df_raw.shape}')\n",
    "print(f'Output shape: {features.shape}')\n",
    "print(f'Features generated: {len(pipeline.feature_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all features with descriptions\n",
    "print('\\n=== FEATURE CATALOG ===')\n",
    "feature_descriptions = {\n",
    "    'consumption_mean': 'Average daily consumption (kWh)',\n",
    "    'consumption_std': 'Standard deviation of daily consumption',\n",
    "    'consumption_median': 'Median daily consumption',\n",
    "    'consumption_cv': 'Coefficient of variation (std/mean)',\n",
    "    'consumption_range': 'Range (max - min)',\n",
    "    'consumption_min': 'Minimum daily consumption',\n",
    "    'consumption_max': 'Maximum daily consumption',\n",
    "    'zero_ratio': 'Proportion of near-zero consumption days',\n",
    "    'sudden_drop_count': 'Number of >50% day-over-day drops',\n",
    "    'sudden_drop_pct': 'Proportion of days with sudden drops',\n",
    "    'max_consecutive_zeros': 'Longest streak of zero consumption',\n",
    "    'autocorrelation': 'Day-over-day consumption correlation',\n",
    "    'consumption_trend': 'Linear trend slope over period',\n",
    "}\n",
    "\n",
    "for i, feat in enumerate(pipeline.feature_names[:15], 1):\n",
    "    desc = feature_descriptions.get(feat, 'See technical documentation')\n",
    "    print(f'{i:2d}. {feat}: {desc}')\n",
    "\n",
    "if len(pipeline.feature_names) > 15:\n",
    "    print(f'    ... and {len(pipeline.feature_names) - 15} additional features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Correlation with Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with theft label\n",
    "if 'FLAG' in features.columns:\n",
    "    feature_cols = [c for c in features.columns if c not in ['CONS_NO', 'FLAG']]\n",
    "    correlations = features[feature_cols + ['FLAG']].corr()['FLAG'].drop('FLAG')\n",
    "    correlations = correlations.sort_values(ascending=False)\n",
    "    \n",
    "    print('=== FEATURE-TARGET CORRELATIONS ===')\n",
    "    print('\\nTop 10 Positive Correlations (associated with theft):')\n",
    "    for feat, corr in correlations.head(10).items():\n",
    "        print(f'  {feat}: {corr:+.4f}')\n",
    "    \n",
    "    print('\\nTop 10 Negative Correlations (associated with normal):')\n",
    "    for feat, corr in correlations.tail(10).items():\n",
    "        print(f'  {feat}: {corr:+.4f}')\n",
    "    \n",
    "    # Visualize\n",
    "    top_features = correlations.abs().sort_values(ascending=True).tail(15)\n",
    "    colors = ['#E94F37' if correlations[f] > 0 else '#2E86AB' for f in top_features.index]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(top_features)), [correlations[f] for f in top_features.index], color=colors)\n",
    "    plt.yticks(range(len(top_features)), top_features.index)\n",
    "    plt.xlabel('Correlation with Theft Label')\n",
    "    plt.title('Top 15 Features by Correlation with Target')\n",
    "    plt.axvline(0, color='black', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ARTIFACTS_DIR / 'feature_correlations.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('\\nFigure saved: feature_correlations.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Flagging\n",
    "\n",
    "Not all customers can be reliably scored. We flag low-confidence records for exclusion or special handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add quality flags\n",
    "features['quality_flag'] = 'HIGH'\n",
    "\n",
    "# Flag customers with excessive missing data\n",
    "missing_ratio = df[consumption_cols].isna().sum(axis=1) / len(consumption_cols)\n",
    "features.loc[missing_ratio > 0.3, 'quality_flag'] = 'LOW'\n",
    "\n",
    "# Flag customers with all-zero consumption (possible inactive accounts)\n",
    "all_zero = (df[consumption_cols] < 0.01).all(axis=1)\n",
    "features.loc[all_zero, 'quality_flag'] = 'INACTIVE'\n",
    "\n",
    "# Flag customers with very short history (cold start)\n",
    "valid_days = (~df[consumption_cols].isna()).sum(axis=1)\n",
    "features.loc[valid_days < config.min_valid_days, 'quality_flag'] = 'COLD_START'\n",
    "\n",
    "print('=== DATA QUALITY DISTRIBUTION ===')\n",
    "print(features['quality_flag'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Artifact Generation and Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed features\n",
    "features.to_parquet(ARTIFACTS_DIR / 'preprocessed.parquet', index=False)\n",
    "print(f'Saved: {ARTIFACTS_DIR / \"preprocessed.parquet\"}')\n",
    "\n",
    "# Save pipeline\n",
    "pipeline.save(str(ARTIFACTS_DIR / 'pipeline_v1.joblib'))\n",
    "print(f'Saved: {ARTIFACTS_DIR / \"pipeline_v1.joblib\"}')\n",
    "\n",
    "# Generate feature schema with hash for versioning\n",
    "feature_schema = {\n",
    "    'version': pipeline.version,\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'n_features': len(pipeline.feature_names),\n",
    "    'feature_names': pipeline.feature_names,\n",
    "    'config': {\n",
    "        'rolling_windows': config.rolling_windows,\n",
    "        'zero_threshold': config.zero_threshold,\n",
    "        'min_valid_days': config.min_valid_days\n",
    "    },\n",
    "    'train_stats': pipeline.train_stats,\n",
    "    'source_file': str(data_file.name),\n",
    "    'n_records': len(features)\n",
    "}\n",
    "\n",
    "# Generate schema hash for change detection\n",
    "schema_str = json.dumps(feature_schema['feature_names'], sort_keys=True)\n",
    "feature_schema['schema_hash'] = hashlib.md5(schema_str.encode()).hexdigest()[:12]\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'feature_schema.json', 'w') as f:\n",
    "    json.dump(feature_schema, f, indent=2)\n",
    "print(f'Saved: {ARTIFACTS_DIR / \"feature_schema.json\"}')\n",
    "print(f'Schema hash: {feature_schema[\"schema_hash\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary report\n",
    "print('\\n' + '='*60)\n",
    "print('PREPROCESSING SUMMARY REPORT')\n",
    "print('='*60)\n",
    "print(f'\\nInput Data:')\n",
    "print(f'  - Source: {data_file.name}')\n",
    "print(f'  - Records: {len(df_raw):,}')\n",
    "print(f'  - Time span: {len(consumption_cols)} days')\n",
    "print(f'\\nOutput Features:')\n",
    "print(f'  - Total features: {len(pipeline.feature_names)}')\n",
    "print(f'  - Records retained: {len(features):,}')\n",
    "print(f'\\nData Quality:')\n",
    "for flag, count in features['quality_flag'].value_counts().items():\n",
    "    print(f'  - {flag}: {count:,} ({count/len(features)*100:.1f}%)')\n",
    "print(f'\\nArtifacts Generated:')\n",
    "print(f'  - preprocessed.parquet')\n",
    "print(f'  - pipeline_v1.joblib')\n",
    "print(f'  - feature_schema.json')\n",
    "print(f'\\nPipeline Version: {pipeline.version}')\n",
    "print(f'Schema Hash: {feature_schema[\"schema_hash\"]}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Limitations and Assumptions\n",
    "\n",
    "### Documented Assumptions\n",
    "\n",
    "1. **Temporal alignment:** All consumption readings are assumed to represent the same 24-hour periods across customers.\n",
    "\n",
    "2. **Missing data mechanism:** Missingness is assumed to be primarily due to communication failures (MCAR/MAR), not theft-related (MNAR).\n",
    "\n",
    "3. **Label reliability:** Historical theft labels, where available, are assumed to be accurate confirmations from field investigations.\n",
    "\n",
    "4. **Stationarity:** Consumption patterns are assumed to be stationary within the observation window. Significant external shocks (e.g., pandemic, major tariff changes) may require model retraining.\n",
    "\n",
    "### Known Limitations\n",
    "\n",
    "1. **Short observation window:** The dataset covers approximately one month, limiting detection of seasonal theft patterns.\n",
    "\n",
    "2. **No external data:** Features do not incorporate weather, holidays, or socioeconomic factors that could improve detection accuracy.\n",
    "\n",
    "3. **Aggregate daily data:** Hourly or 15-minute granularity would enable detection of time-of-use manipulation.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step:** Proceed to 02_Customer_Clustering_and_Profiles.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
